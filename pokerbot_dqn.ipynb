{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rlcard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6zpZpdJc4kc",
        "outputId": "43968bea-e638-4689-a761-a983c1de42aa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rlcard in /usr/local/lib/python3.8/dist-packages (1.0.9)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from rlcard) (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.8/dist-packages (from rlcard) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "r2ORu3snc5N8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    ''' \n",
        "    Replay mem for saving transitions\n",
        "    '''\n",
        "    def __init__(self, cap, batch_size):\n",
        "        ''' \n",
        "        Initialize ReplayMemory\n",
        "\n",
        "        :param int cap: the size of the mem buffer\n",
        "        :param int batch_size: the size of the batches\n",
        "        '''\n",
        "        self.cap = cap\n",
        "        self.mem = []\n",
        "        self.pos = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        '''\n",
        "        Save a transition into mem\n",
        "        '''\n",
        "        if len(self.mem) < self.cap:\n",
        "            self.mem.append(None)\n",
        "        self.mem[self.pos] = Transition(*args)\n",
        "        self.pos = (self.pos + 1) % self.cap\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''\n",
        "        Choose random sample from the mem with size of the batch size\n",
        "        '''\n",
        "        items = random.choice(self.mem, batch_size)\n",
        "        return map(np.array, zip(*items))\n"
      ],
      "metadata": {
        "id": "ZqNMefQLc-vA"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class dqn_l(object):\n",
        "    '''\n",
        "    Deep Q-Network\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_states=36, num_actions=4, hid_layer=[64, 32], lr=0.001, dev=None):\n",
        "      \n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.hid_layer = hid_layer\n",
        "        self.lr=lr\n",
        "        self.dev = dev\n",
        "\n",
        "        # DQN network based on the layers\n",
        "        layers = self.num_states + self.hid_layer\n",
        "        dqn_l = [nn.Flatten()]\n",
        "        dqn_l.append(nn.BatchNorm1d(layers[0]))\n",
        "        for i in range(len(layers)-1):\n",
        "            dqn_l.append(nn.Linear(layers[i], layers[i+1], bias=True))\n",
        "            dqn_l.append(nn.Tanh())\n",
        "        dqn_l.append(nn.Linear(layers[-1], self.num_actions, bias=True))\n",
        "        dqn_l = nn.Sequential(*dqn_l)\n",
        "\n",
        "        dqn_l = dqn_l.to(self.dev)\n",
        "        self.dqn_l = dqn_l\n",
        "        self.dqn_l.eval()\n",
        "\n",
        "        # Initialize weights in the network\n",
        "        for p in self.dqn_l.parameters():\n",
        "            if len(p.data.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p.data)\n",
        "\n",
        "        # Define loss function\n",
        "        self.loss_func = nn.MSELoss(reduction='mean')\n",
        "\n",
        "        # Define optimizer\n",
        "        \n",
        "        self.optimizer = torch.optim.RMSprop(self.dqn_l.parameters())\n",
        "\n",
        "\n",
        "    def get_qvalue(self, nxt_state):\n",
        "        \n",
        "        # Disable gradient calculation\n",
        "        with torch.no_grad():\n",
        "            # Create torch tensor\n",
        "            nxt_state = torch.from_numpy(nxt_state).float().to(self.dev)\n",
        "            # Get Q values\n",
        "            q_val = self.dqn_l(nxt_state).cpu().numpy()\n",
        "        return q_val\n",
        "\n",
        "    def update(self, state_batch, action_batch, target_batch):\n",
        "        ''' \n",
        "        Update the policy network\n",
        "\n",
        "        :param np.ndarray state_batch: Batch of states from replay memory\n",
        "        :param np.ndarray action_batch: Batch of actions from replay memory\n",
        "        :param np.ndarray target_batch: Batch of Q-values from the target policy, it used during the optimization step\n",
        "        :return float batch_loss: The calculated loss on the batch       \n",
        "        '''\n",
        "        # Set the gradients to zero\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Set the network in training mode\n",
        "        self.dqn_l.train()\n",
        "\n",
        "        # Create torch tensors\n",
        "        state_batch = torch.from_numpy(state_batch).float().to(self.dev)\n",
        "        action_batch = torch.from_numpy(action_batch).long().to(self.dev)\n",
        "        target_batch = torch.from_numpy(target_batch).float().to(self.dev)\n",
        "\n",
        "        # Gather Q-values from network and replay memory actions\n",
        "        q_val = torch.gather(self.dqn_l(state_batch), dim=-1, index=action_batch.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Optimization step\n",
        "        batch_loss = self.loss_func(q_val, target_batch)\n",
        "        batch_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        batch_loss = batch_loss.item()\n",
        "        self.dqn_l.eval()\n",
        "        return batch_loss"
      ],
      "metadata": {
        "id": "jkkdeScRdFGT"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "class DQN_agent(object):\n",
        "    '''\n",
        "    DQN agent\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                state_no,\n",
        "                num_act,\n",
        "                extra_act=0,\n",
        "                replay_memory_capacity=20000,\n",
        "                min_samp=1000,\n",
        "                b_s=16,\n",
        "                train_time=1,\n",
        "                df=0.99,\n",
        "                hidden_layers=[64, 32],\n",
        "                learning_rate=0.0001,\n",
        "                eps=20000,\n",
        "                upd_target=1000, \n",
        "                device=None):\n",
        "\n",
        "      \n",
        "        \n",
        "        self.min_samp = min_samp\n",
        "        self.upd_target = upd_target\n",
        "        self.df = df\n",
        "        self.eps = eps\n",
        "        self.b_s = b_s\n",
        "        self.num_act = num_act\n",
        "        self.train_time = train_time\n",
        "        self.extra_act = extra_act\n",
        "\n",
        "       \n",
        "        if device is None:\n",
        "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        else:\n",
        "            self.device = device\n",
        "\n",
        "        # Create the replay memory\n",
        "        self.memory = ReplayMemory(replay_memory_capacity, b_s)\n",
        "\n",
        "        # Initialize current timestep and current training timestep\n",
        "        self.current_timestep, self.current_training_timestep = 0, 0\n",
        "\n",
        "        # Create array for the eps values during the eps decay \n",
        "        self.epsilons = np.linspace(1.0, 0.1, eps)\n",
        "\n",
        "        # Create the policy and the target network\n",
        "        self.policy_dqn = dqn_l(num_act=num_act, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n",
        "        self.target_dqn = dqn_l(num_act=num_act, learning_rate=learning_rate, state_no=state_no, hidden_layers=hidden_layers, device=self.device)\n",
        "\n",
        "        # Set use_raw value for the RLCard environment\n",
        "        self.use_raw = False\n",
        "\n",
        "    def store_and_train(self, transition):\n",
        "       \n",
        "        self.current_timestep += 1\n",
        "        # Train the agent if the replay memory has data already and agent reached the next training period\n",
        "        time_between = self.current_timestep - self.min_samp\n",
        "        if time_between>=0 and time_between%self.train_time == 0:\n",
        "            self.train()\n",
        "\n",
        "    def discard_invalid_actions(self, action_probs, valid_actions):\n",
        "        ''' \n",
        "        Remove invalid actions and normalize the probabilities.\n",
        "\n",
        "        :param numpy.array[float] action_probs: Probabilities of all action\n",
        "        :param list[int] valid_actions: Valid actions in the current state\n",
        "        :return numpy.array[float] normalised_probs: Probabilities of valid actions\n",
        "        '''\n",
        "        valid_actions=[0,1,2]\n",
        "        # Initialize new array\n",
        "        normalised_probs = np.zeros(action_probs.shape[0])\n",
        "        # Add probability values of valid actions to the array\n",
        "        normalised_probs[valid_actions] = action_probs[valid_actions]\n",
        "        # Normalize probabilities\n",
        "        normalised_probs[valid_actions] = 1 / len(valid_actions)\n",
        "        return normalised_probs\n",
        "\n",
        "    def predict(self, state):\n",
        "       \n",
        "        eps = self.epsilons[min(self.current_timestep, self.eps-1)]\n",
        "        actions = np.ones(self.num_act, dtype=float) * eps / self.num_act\n",
        "        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state, 0))[0]\n",
        "        best_action = np.argmax(q_values)\n",
        "        actions[best_action] += (1.0 - eps)\n",
        "        return actions\n",
        "\n",
        "    def step(self, state):\n",
        "      \n",
        "        actions = self.predict(state['obs'])\n",
        "        normalised_probs = self.discard_invalid_actions(actions, state['legal_actions'])\n",
        "        action = np.random.choice(np.arange(len(actions)), p=normalised_probs)\n",
        "        return action\n",
        "\n",
        "\n",
        "    def eval_step(self, state):\n",
        "       \n",
        "        q_values = self.policy_dqn.get_qvalue(np.expand_dims(state['obs'], 0))[0]\n",
        "        normalised_probs = self.discard_invalid_actions(np.exp(q_values), state['legal_actions'])\n",
        "        # Check version of choosing action\n",
        "        if self.extra_act == 1:\n",
        "          # If Raise (1) is a valid action and the best action is Call (0)\n",
        "          if 1 in state['legal_actions'] and np.argmax(normalised_probs)==0:\n",
        "            best_action = 1\n",
        "          else:\n",
        "            best_action = np.argmax(normalised_probs)\n",
        "        elif self.extra_act == 2:\n",
        "          # If Raise (1) is a valid action and the best action is Check (3)\n",
        "          if 1 in state['legal_actions'] and np.argmax(normalised_probs)==3:\n",
        "            best_action = 1\n",
        "          else:\n",
        "            best_action = np.argmax(normalised_probs)\n",
        "        elif self.extra_act == 3:\n",
        "          # If Raise (1) is a valid action and the best action is Fold (2)\n",
        "          if 1 in state['legal_actions'] and np.argmax(normalised_probs)==2:\n",
        "            best_action = 1\n",
        "          else:\n",
        "            best_action = np.argmax(normalised_probs)\n",
        "        else:\n",
        "          best_action = np.argmax(normalised_probs)\n",
        "        return best_action, normalised_probs\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "       \n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(self.b_s)\n",
        "\n",
        "        # Get best next action using the policy network\n",
        "        q_values_next = self.policy_dqn.get_qvalue(next_state_batch)\n",
        "        best_actions = np.argmax(q_values_next, axis=1)\n",
        "\n",
        "        # Calculate Q values from the target policy\n",
        "        q_values_next_target = self.target_dqn.get_qvalue(next_state_batch)\n",
        "        target_batch = reward_batch + np.invert(done_batch).astype(np.float32) * self.df * q_values_next_target[np.arange(self.b_s), best_actions]\n",
        "\n",
        "        # Update policy network\n",
        "        state_batch = np.array(state_batch)\n",
        "        loss = self.policy_dqn.update(state_batch, action_batch, target_batch)\n",
        "\n",
        "        # Update target network based on the target update period\n",
        "        if self.current_training_timestep % self.upd_target == 0:\n",
        "            self.target_dqn = deepcopy(self.policy_dqn)\n",
        "\n",
        "        self.current_training_timestep += 1\n",
        "\n",
        "\n",
        "    def get_state_dict(self):\n",
        "       \n",
        "        model_dict = {'policy_network': self.policy_dqn.dqn_l.state_dict(), 'target_network': self.target_dqn.dqn_l.state_dict()}\n",
        "        return model_dict\n",
        "\n",
        "    def load_networks(self, checkpoint):\n",
        "       \n",
        "        self.policy_dqn.dqn_l.load_state_dict(checkpoint['policy_network'])\n",
        "        self.target_dqn.dqn_l.load_state_dict(checkpoint['target_network'])\n"
      ],
      "metadata": {
        "id": "EuQfE_44dKuw"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rlcard\n",
        "from rlcard import models\n",
        "from rlcard.agents import RandomAgent\n",
        "from rlcard.utils import seeding, tournament\n",
        "from rlcard.utils import Logger\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Create environments\n",
        "env = rlcard.make('limit-holdem', config={'seed': 0})\n",
        "eval_env = rlcard.make('limit-holdem', config={'seed': 0})\n",
        "\n",
        "# Set a global seed\n",
        "seeding.create_seed(0)\n",
        "\n",
        "# Play agressive game based on the version of choosing actual action\n",
        "# Action with maximum value: 0\n",
        "# Raise action instead of Call if possible: 1\n",
        "# Raise action instead of Check if possible: 2\n",
        "# Raise action instead of Fold if possible: 3\n",
        "extra_action_version=1\n",
        "\n",
        "\n",
        "\n",
        "# Create DQN agent\n",
        "agent = DQN_agent(state_no=[72],\n",
        "                  act_no=4, \n",
        "                  replay_memory_min_sample=1000,\n",
        "                  training_period=10,\n",
        "                  hidden_layers=[128, 128],\n",
        "                  device=torch.device('cpu'),\n",
        "                  extra_action_version=extra_action_version)\n",
        "\n",
        "# Create random opponent agent\n",
        "random_agent = RandomAgent(num_actions=len(eval_env.actions))\n",
        "\n",
        "# Add the agent to the environments\n",
        "env.set_agents([agent, random_agent])\n",
        "eval_env.set_agents([agent, random_agent])\n",
        "\n",
        "\n",
        "\n",
        "# Number of episodes, number of games during evaluation and evaluation in every N steps\n",
        "episode_no, evaluate_games, evaluate_period = 100, 50, 10\n",
        "\n",
        "for episode in range(episode_no):\n",
        "    # Generate data from the environment\n",
        "    trajectories, _ = env.run(is_training=True)\n",
        "\n",
        "    # Feed transitions into agent memory, and train the agent\n",
        "    for ts in trajectories[0]:\n",
        "        agent.store_and_train(ts)\n",
        "    reward=tournament(eval_env,evaluate_games)\n",
        "    print(reward)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cizi__8dZdD",
        "outputId": "37563eaa-6223-45e4-9bcc-b9f2ba712dfa"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3.67, -3.67]\n",
            "[2.67, -2.67]\n",
            "[2.59, -2.59]\n",
            "[3.76, -3.76]\n",
            "[3.23, -3.23]\n",
            "[3.04, -3.04]\n",
            "[2.77, -2.77]\n",
            "[3.57, -3.57]\n",
            "[3.51, -3.51]\n",
            "[2.96, -2.96]\n",
            "[2.5, -2.5]\n",
            "[3.24, -3.24]\n",
            "[2.55, -2.55]\n",
            "[4.31, -4.31]\n",
            "[2.42, -2.42]\n",
            "[2.19, -2.19]\n",
            "[3.04, -3.04]\n",
            "[2.5, -2.5]\n",
            "[2.83, -2.83]\n",
            "[3.6, -3.6]\n",
            "[2.69, -2.69]\n",
            "[3.54, -3.54]\n",
            "[2.96, -2.96]\n",
            "[2.9, -2.9]\n",
            "[2.65, -2.65]\n",
            "[3.48, -3.48]\n",
            "[2.75, -2.75]\n",
            "[2.06, -2.06]\n",
            "[3.1, -3.1]\n",
            "[2.23, -2.23]\n",
            "[3.11, -3.11]\n",
            "[3.16, -3.16]\n",
            "[2.14, -2.14]\n",
            "[2.56, -2.56]\n",
            "[1.94, -1.94]\n",
            "[2.94, -2.94]\n",
            "[3.51, -3.51]\n",
            "[2.98, -2.98]\n",
            "[3.42, -3.42]\n",
            "[3.33, -3.33]\n",
            "[3.24, -3.24]\n",
            "[3.05, -3.05]\n",
            "[2.27, -2.27]\n",
            "[2.24, -2.24]\n",
            "[1.53, -1.53]\n",
            "[2.97, -2.97]\n",
            "[3.51, -3.51]\n",
            "[1.45, -1.45]\n",
            "[1.96, -1.96]\n",
            "[3.7, -3.7]\n",
            "[3.18, -3.18]\n",
            "[2.51, -2.51]\n",
            "[3.13, -3.13]\n",
            "[4.88, -4.88]\n",
            "[3.73, -3.73]\n",
            "[2.81, -2.81]\n",
            "[2.62, -2.62]\n",
            "[2.45, -2.45]\n",
            "[3.05, -3.05]\n",
            "[2.34, -2.34]\n",
            "[2.74, -2.74]\n",
            "[2.32, -2.32]\n",
            "[3.28, -3.28]\n",
            "[2.99, -2.99]\n",
            "[3.45, -3.45]\n",
            "[3.94, -3.94]\n",
            "[2.8, -2.8]\n",
            "[2.34, -2.34]\n",
            "[4.12, -4.12]\n",
            "[3.21, -3.21]\n",
            "[3.43, -3.43]\n",
            "[4.22, -4.22]\n",
            "[3.31, -3.31]\n",
            "[2.74, -2.74]\n",
            "[3.73, -3.73]\n",
            "[3.47, -3.47]\n",
            "[2.48, -2.48]\n",
            "[2.51, -2.51]\n",
            "[3.09, -3.09]\n",
            "[2.36, -2.36]\n",
            "[2.5, -2.5]\n",
            "[2.6, -2.6]\n",
            "[1.82, -1.82]\n",
            "[3.8, -3.8]\n",
            "[3.16, -3.16]\n",
            "[4.26, -4.26]\n",
            "[2.5, -2.5]\n",
            "[1.74, -1.74]\n",
            "[3.49, -3.49]\n",
            "[3.07, -3.07]\n",
            "[3.14, -3.14]\n",
            "[2.96, -2.96]\n",
            "[3.32, -3.32]\n",
            "[3.28, -3.28]\n",
            "[3.12, -3.12]\n",
            "[1.49, -1.49]\n",
            "[3.41, -3.41]\n",
            "[2.45, -2.45]\n",
            "[3.6, -3.6]\n",
            "[2.11, -2.11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2DAXuo_0SkP"
      },
      "execution_count": 163,
      "outputs": []
    }
  ]
}